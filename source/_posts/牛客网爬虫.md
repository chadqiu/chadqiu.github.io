---
title: 牛客网爬虫
date: 2023-01-27 16:43:42
categories:
  - tools
    - 爬虫
tags:
  - 爬虫 
  - python
  - 牛客网
toc: true # 是否启用内容索引
---

爬取牛客网的帖子，获取一些感兴趣的信息，比如职位内推等，使用增量更新，老数据保存到mysql数据库中用于去重，每日新增的数据发邮件提醒，效果如下图所示:


![邮件截图](https://img-blog.csdnimg.cn/dc63410d0da4456e836c9a4ccfcd338e.png)

## MySQL数据表结构如下

```sql
CREATE TABLE IF NOT EXISTS `newcoder_search`(
   `id` BIGINT NOT NULL,
   `title` VARCHAR(40),
   `content` text NOT NULL,
   `user` VARCHAR(40) NOT NULL,
   `url` VARCHAR(60) NOT NULL,
   `created_time` datetime NOT NULL,
   `edited_time` datetime NOT NULL,
   PRIMARY KEY ( `id` )
)ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

## python爬虫代码如下

```python
# -*- coding: utf-8 -*-
import requests
import json
import time
import re
import pymysql
import smtplib
from email.mime.text import MIMEText

def get_newcoder_page(page = 1, keyword = "补录"):
    header = {
        "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36",
        "content-type":"application/json"
    }
    data = {
        "type": "all", 
        "query": keyword, 
        "page": page, 
        "tag": [], 
        "order": "create"
    }
    x = requests.post('https://gw-c.nowcoder.com/api/sparta/pc/search', data = json.dumps(data), headers = header, )
    return x.json()

def parse_newcoder_page(data, skip_words = [], start_date = '2023'):
    assert data['success'] == True
    pattern = re.compile("|".join(skip_words)) 
    res = []
    for x in data['data']['records']:
        x = x['data']
        dic = {"user": x['userBrief']['nickname']}
        
        x = x['contentData'] if 'contentData' in x else x['momentData']
        dic['title'] = x['title']
        dic['content'] = x['content']
        dic['id'] = int(x['id'])
        dic['url'] = 'https://www.nowcoder.com/discuss/' + str(x['id'])
        
        if len(skip_words) > 0 and pattern.search(x['title'] + x['content']) != None:
            continue
  
        createdTime = x['createdAt'] if 'createdAt' in x else x['createTime']
        dic['createTime'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(createdTime // 1000))
        dic['editTime'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(x['editTime'] // 1000))
        
        if dic['editTime'] < start_date: continue
        res.append(dic)
        
    return res

def upsert_to_db(data, host, user, passwd, database,charset, port):
    db = pymysql.connect(
          host=host,
          user=user,
          passwd=passwd,
          database = database,
          charset=charset,
          port = port
    )
    try:
        cursor = db.cursor()  
        sql = "select id, edited_time from newcoder_search where id in ({})".format(",".join([str(x['id']) for x in data]))
        cursor.execute(sql)
        an = cursor.fetchall()
        dic = {x[0] : x[1].strftime("%Y-%m-%d %H:%M:%S") for x in an}

        insert_data = [[x[k] for k in x] for x in data if x['id'] not in dic]
        update_data = [(x['editTime'], x['id']) for x in data if x['id'] in dic and dic[x['id']] != x['editTime']]
        sql = "INSERT INTO newcoder_search (user, title, content, id, url, created_time, edited_time) VALUES(%s, %s, %s, %s, %s, %s, %s)"
        cursor.executemany(sql, insert_data)
        sql = "update newcoder_search set edited_time = %s where id = %s"
        cursor.executemany(sql, update_data)
        db.commit()
    except Exception as e:
        print("db error: ", e)
    db.close()
    return [x for x in data if x['id'] not in dic], [x for x in data if x['id'] in dic and dic[x['id']] != x['editTime']]

def table_html_generate(data):
    s = '<table>'
    s += '<tr>' + "\n".join(["<th>" + x + '</th>' for x in data[0]]) + '</tr>'
    for d in data:
        s += '<tr>' + "\n".join(["<td>" + str(d[x]) + '</td>' for x in d]) + '</tr>'
    s += '</table>'
    return s

def send_email(insert_data, update_data, mail_host, mail_user, mail_pass, sender, receivers):
    msg = ''
    if len(insert_data) > 0:
        msg += '<h1>insert</h1></br>' + table_html_generate(insert_data) + '</br></br>'
    if len(update_data) > 0:
        msg += '<h1>update</h1></br>' + table_html_generate(update_data) + '</br></br>'
    if msg == '':
        msg = '<h1>今日无新增数据</h1></br>'
        
    message = MIMEText(msg, 'html', 'utf-8')
    message['Subject'] = '牛客网{}招聘信息'.format(time.strftime("%Y-%m-%d"))
    message['From'] = sender
    message['To'] = receivers[0]
    try:
        smtpObj = smtplib.SMTP_SSL(mail_host, 465)
        #smtpObj.connect(mail_host, 465)
        smtpObj.login(mail_user, mail_pass)
        smtpObj.sendmail(
            sender, receivers, message.as_string())
        smtpObj.quit()
        return True
    except smtplib.SMTPException as e:
        print('email send error: ', e)  
        return False    

def run(keywords, skip_words, db_config, mail_config = None):
    res = []
    for key in keywords:
        print(key, time.strftime("%Y-%m-%d %H:%M:%S"))
        for i in range(1, 11):
            print(i)
            a = get_newcoder_page(i, key)
            b = parse_newcoder_page(a, skip_words)
            if b == None or len(b) < 1: break
            res.extend(b)
            time.sleep(1)
    
    result, ids = [], set()  # 去重
    for x in res:
        if x['id'] in ids: continue
        ids.add(x['id'])
        result.append(x)

    print("total num: ", len(result))
    x = upsert_to_db(result, **db_config)  # insert_data, update_data
    if mail_config:
        send_email(*x, **mail_config)

def main(): 
    # 指定要过滤的词
    skip_words=['求捞', '泡池子', '池子了', '池子中', 'offer对比', '总结一下', '给个建议', '开奖群', '没消息', '有消息', '拉垮', '求一个', '求助', '池子的', '决赛圈', 'offer比较', '求捞', '补录面经', '捞捞', '收了我吧', 'offer选择', '有offer了', '想问一下', 'kpi吗', 'kpi面吗', 'kpi面吧']
    
    # 指定搜索的关键词
    keywords = ['补招', '补录']
    
    #配置数据库信息
    db_config = {
        "host" : "localhost", 
        "user" : "root", 
        "passwd" : "your password", 
        "database" : 'your database',
        "charset" : 'utf8', 
        "port": your mysql port
    }
    
    # 配置邮箱信息
    mail_config = {
        "mail_host" : 'smtp server host', 
        "mail_user" : 'your user name', 
        "mail_pass" : 'password',   # 密码(部分邮箱为授权码)
        "sender" : 'sender email', 
        "receivers" : ["receivers email"]
    }
    
    run(keywords, skip_words, db_config, mail_config)
    

if __name__ == "__main__":
    main()
    print("end")
```

## 内容筛选

直接爬下来的帖子既包含招聘信息，又包含求职者发布的面经、讨论贴等其他内容，现在希望过滤掉那些无关的帖子。
目前只写了一个根据关键词进行过滤的功能，用户指定`skip_words`，凡包含这里面的关键词的都会被过滤。
但这种方法也不是很准，还是会有漏网之鱼，怎么才能实现更精准的过滤呢？
一种可能的方案是训练一个NLP分类模型进行过滤，但这需要大量数据进行训练，目前我已爬取牛客网上历史数据4万多条，但需要标注数据，不太想人工去标数据，这个计划暂时搁置，代码和历史数据已开源在[github  newcoder-crawler](https://github.com/chadqiu/newcoder-crawler)

## 编写shell脚本，使用crontab自动运行

shell脚本newcoder.sh内容如下：

```shell
source /root/anaconda3/bin/activate base
cd /root/chadqiu/crawler
python newcoder.py > server.log 2>&1
```

## crontab配置

```shell
crontab -l  # 查看已经存在的定时任务
crontab -e  #编辑/新加定时任务
service crond restart  #重启，是刚才的配置更改生效
```

这里crontab -e新加配置内容如下，每天18：30运行一次：

```shell
30 18 * * * bash /root/chadqiu/crawler/newcoder.sh
```

**cron配置语法规则**：
5个位置含义如下：

```shell
      Minute Hour Day  Month    Dayofweek   command
     分钟     小时   天     月       天每星期       命令
```

```shell
 “*”代表取值范围内的数字,
 “/”代表”每”,
 “-”代表从某个数字到某个数字,
 “,”分开几个离散的数字
```
